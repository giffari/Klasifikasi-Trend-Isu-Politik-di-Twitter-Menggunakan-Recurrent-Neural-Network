{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "data = pd.read_csv('tweets_10000.csv')\n",
    "reviews = np.array(data['tweet_text'])[:10000]\n",
    "labels = np.array(data['label'])[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>tweet_lang</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>user_since</th>\n",
       "      <th>user_time_zone</th>\n",
       "      <th>user_verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Mon Jul 13 13:32:54 +0000 2020</td>\n",
       "      <td>1282669333798240257</td>\n",
       "      <td>*Maksudnya uang dibalik meja,Bisa di katakan K...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>in</td>\n",
       "      <td>1281767862131023872</td>\n",
       "      <td>SinoJardin</td>\n",
       "      <td>JardinSino</td>\n",
       "      <td>Sat Jul 11 01:51:12 +0000 2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Mon Jul 13 13:32:53 +0000 2020</td>\n",
       "      <td>1282669329645879297</td>\n",
       "      <td>#BURUHTOLAKAKSI16JULI SIKAP BURUH KERAS MENYAT...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>in</td>\n",
       "      <td>1224937759678193664</td>\n",
       "      <td>Ferdy Simbolon</td>\n",
       "      <td>SimbolonFerdy</td>\n",
       "      <td>Wed Feb 05 06:08:40 +0000 2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Jul 13 13:32:51 +0000 2020</td>\n",
       "      <td>1282669320779124736</td>\n",
       "      <td>Ahh sama je la semuanya bodo baik muda ke tua ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>in</td>\n",
       "      <td>353955052</td>\n",
       "      <td>áµƒË¢ ðŸ‡²ðŸ‡¾</td>\n",
       "      <td>apik1323</td>\n",
       "      <td>Fri Aug 12 22:20:51 +0000 2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Jul 13 13:32:51 +0000 2020</td>\n",
       "      <td>1282669319445286914</td>\n",
       "      <td>#BURUHTOLAKAKSI16JULI SIKAP BURUH KERAS MENYAT...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>in</td>\n",
       "      <td>1268840238765404160</td>\n",
       "      <td>Nikita fris</td>\n",
       "      <td>fris_niki</td>\n",
       "      <td>Fri Jun 05 09:41:14 +0000 2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Mon Jul 13 13:32:47 +0000 2020</td>\n",
       "      <td>1282669304714940416</td>\n",
       "      <td>#BURUHTOLAKAKSI16JULI SIKAP BURUH KERAS MENYAT...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>in</td>\n",
       "      <td>1207860942240804865</td>\n",
       "      <td>Randi Radika</td>\n",
       "      <td>radika_randi</td>\n",
       "      <td>Fri Dec 20 03:11:15 +0000 2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                      created_at             tweet_id  \\\n",
       "0      2  Mon Jul 13 13:32:54 +0000 2020  1282669333798240257   \n",
       "1      2  Mon Jul 13 13:32:53 +0000 2020  1282669329645879297   \n",
       "2      0  Mon Jul 13 13:32:51 +0000 2020  1282669320779124736   \n",
       "3      0  Mon Jul 13 13:32:51 +0000 2020  1282669319445286914   \n",
       "4      2  Mon Jul 13 13:32:47 +0000 2020  1282669304714940416   \n",
       "\n",
       "                                          tweet_text  retweet_count  \\\n",
       "0  *Maksudnya uang dibalik meja,Bisa di katakan K...              0   \n",
       "1  #BURUHTOLAKAKSI16JULI SIKAP BURUH KERAS MENYAT...              0   \n",
       "2  Ahh sama je la semuanya bodo baik muda ke tua ...              0   \n",
       "3  #BURUHTOLAKAKSI16JULI SIKAP BURUH KERAS MENYAT...              0   \n",
       "4  #BURUHTOLAKAKSI16JULI SIKAP BURUH KERAS MENYAT...              0   \n",
       "\n",
       "   favorite_count tweet_lang              user_id       user_name  \\\n",
       "0               0         in  1281767862131023872      SinoJardin   \n",
       "1               0         in  1224937759678193664  Ferdy Simbolon   \n",
       "2               0         in            353955052           áµƒË¢ ðŸ‡²ðŸ‡¾   \n",
       "3               0         in  1268840238765404160     Nikita fris   \n",
       "4               0         in  1207860942240804865    Randi Radika   \n",
       "\n",
       "  user_screen_name                      user_since  user_time_zone  \\\n",
       "0       JardinSino  Sat Jul 11 01:51:12 +0000 2020             NaN   \n",
       "1    SimbolonFerdy  Wed Feb 05 06:08:40 +0000 2020             NaN   \n",
       "2         apik1323  Fri Aug 12 22:20:51 +0000 2011             NaN   \n",
       "3        fris_niki  Fri Jun 05 09:41:14 +0000 2020             NaN   \n",
       "4     radika_randi  Fri Dec 20 03:11:15 +0000 2019             NaN   \n",
       "\n",
       "   user_verified  \n",
       "0          False  \n",
       "1          False  \n",
       "2          False  \n",
       "3          False  \n",
       "4          False  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 3369, 0: 3378, 1: 3253})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview counter\n",
    "# 1 = Positif; 2 = Negatif; 0 = Netral\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "# padding sequences\n",
    "def pad_features(reviews_ints, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features\n",
    "\n",
    "# tokenizer\n",
    "def tokenize_review(test_review):\n",
    "    test_review = test_review.lower() # lowercase\n",
    "    # get rid of punctuation\n",
    "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "\n",
    "    # splitting by spaces\n",
    "    test_words = test_text.split()\n",
    "    \n",
    "    # get rid of web address, twitter id, and digit\n",
    "    new_text = []\n",
    "    for word in test_words:\n",
    "        if (word[0] != '@') & ('http' not in word) & (~word.isdigit()):\n",
    "            new_text.append(word)\n",
    "\n",
    "    # tokens\n",
    "    test_ints = []\n",
    "    test_ints.append([vocab_to_int[word] for word in new_text])\n",
    "\n",
    "    return test_ints\n",
    "\n",
    "# prediction\n",
    "def predict(net, test_review, sequence_length=30):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    # tokenize review\n",
    "    test_ints = tokenize_review(test_review)\n",
    "    \n",
    "    # pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    features = pad_features(test_ints, seq_length)\n",
    "    \n",
    "    # convert to tensor to pass into your model\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    \n",
    "    # get the output from the model\n",
    "    output, h = net(feature_tensor, h)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze()) \n",
    "    # printing output value, before rounding\n",
    "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
    "    \n",
    "    # print custom response\n",
    "    if(pred.item()==1):\n",
    "        print(\"Positif.\")\n",
    "    elif(pred.item()==0):\n",
    "        print(\"Netral.\")\n",
    "    else:\n",
    "        print(\"Negatif.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(8000, 30) \n",
      "Validation set: \t(1000, 30) \n",
      "Test set: \t\t(1000, 30)\n"
     ]
    }
   ],
   "source": [
    "# pre-processing\n",
    "punctuation = '!\"#$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~'\n",
    "\n",
    "# get rid of punctuation\n",
    "all_reviews = 'separator'.join(reviews)\n",
    "all_reviews = all_reviews.lower()\n",
    "all_text = ''.join([c for c in all_reviews if c not in punctuation])\n",
    "\n",
    "# split by new lines and spaces\n",
    "reviews_split = all_text.split('separator')\n",
    "all_text = ' '.join(reviews_split)\n",
    "\n",
    "# create a list of words\n",
    "words = all_text.split()\n",
    "# get rid of web address, twitter id, and digit\n",
    "\n",
    "new_reviews = []\n",
    "for review in reviews_split:\n",
    "    review = review.split()\n",
    "    new_text = []\n",
    "    for word in review:\n",
    "        if (word[0] != '@') & ('http' not in word) & (~word.isdigit()):\n",
    "            new_text.append(word)\n",
    "    new_reviews.append(new_text)\n",
    "    \n",
    "## Build a dictionary that maps words to integers\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "## use the dict to tokenize each review in reviews_split\n",
    "## store the tokenized reviews in reviews_ints\n",
    "reviews_ints = []\n",
    "for review in new_reviews:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in review])\n",
    "    \n",
    "# 1 = positif, 2 = negatif, 0 = netral label conversion\n",
    "encoded_labels = []\n",
    "for label in labels:\n",
    "    if label == 1:\n",
    "        encoded_labels.append(1)\n",
    "    elif label == 0:\n",
    "        encoded_labels.append(0)\n",
    "    else:\n",
    "        encoded_labels.append(2)\n",
    "\n",
    "encoded_labels = np.asarray(encoded_labels)\n",
    "\n",
    "# test implementation!\n",
    "seq_length = 30\n",
    "features = pad_features(reviews_ints, seq_length=seq_length)\n",
    "\n",
    "## test statements \n",
    "assert len(features)==len(reviews_ints), \"The features should have as many rows as reviews.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "split_frac = 0.8\n",
    "split_idx = int(len(features)*split_frac)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of the resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n",
    "\n",
    "\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure the SHUFFLE the training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU.\n"
     ]
    }
   ],
   "source": [
    "# preparing network\n",
    "\n",
    "# first checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')\n",
    "    \n",
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
    "output_size = 1\n",
    "embedding_dim = 200\n",
    "hidden_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Step: 100... Loss: -1.233672... Val Loss: -0.010508\n",
      "Epoch: 2/10... Step: 200... Loss: 0.266463... Val Loss: -0.011516\n",
      "Epoch: 2/10... Step: 300... Loss: 0.082972... Val Loss: -0.007509\n",
      "Epoch: 3/10... Step: 400... Loss: -1.080732... Val Loss: 0.032306\n",
      "Epoch: 4/10... Step: 500... Loss: 0.195846... Val Loss: 0.196909\n",
      "Epoch: 4/10... Step: 600... Loss: -0.722359... Val Loss: -0.260807\n",
      "Epoch: 5/10... Step: 700... Loss: -22.557310... Val Loss: -0.325737\n",
      "Epoch: 5/10... Step: 800... Loss: -24.657799... Val Loss: 0.469900\n",
      "Epoch: 6/10... Step: 900... Loss: -18.267254... Val Loss: -0.028774\n",
      "Epoch: 7/10... Step: 1000... Loss: -31.811924... Val Loss: -0.307216\n",
      "Epoch: 7/10... Step: 1100... Loss: -25.266123... Val Loss: -0.037911\n",
      "Epoch: 8/10... Step: 1200... Loss: -28.250751... Val Loss: -0.320075\n",
      "Epoch: 9/10... Step: 1300... Loss: -31.906994... Val Loss: 0.987543\n",
      "Epoch: 9/10... Step: 1400... Loss: -39.721298... Val Loss: -0.428554\n",
      "Epoch: 10/10... Step: 1500... Loss: -18.023981... Val Loss: -0.471974\n",
      "Epoch: 10/10... Step: 1600... Loss: -23.886909... Val Loss: 0.743120\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "epochs = 10 \n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: -1.489\n",
      "Test accuracy: 0.349\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "\n",
    "# get test data loss and accuracy\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting\n",
    "\n",
    "# negative test review\n",
    "test_review = \"jangan jadikan politik sebagai alat bung\"\n",
    "# test code and generate tokenized review\n",
    "test_ints = tokenize_review(test_review)\n",
    "# test sequence padding\n",
    "seq_length=30\n",
    "features = pad_features(test_ints, seq_length)\n",
    "# test conversion to tensor and pass into your model\n",
    "feature_tensor = torch.from_numpy(features)\n",
    "seq_length = 30 # good to use the length that was trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 1.000000\n",
      "Positif.\n"
     ]
    }
   ],
   "source": [
    "# call function on negative review\n",
    "test_review_neg = \"@Bunnga8 Ini politik adu domba, eh bukan deng. Pokonya itulah istilahnya\"\n",
    "predict(net, test_review_neg, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 1.000000\n",
      "Positif.\n"
     ]
    }
   ],
   "source": [
    "# call function on positive review\n",
    "test_review_pos = \"@collegemenfess Hmm semua akan tersasa tantangannya kalo udah bahas politik\"\n",
    "predict(net, test_review_pos, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.084279\n",
      "Netral.\n"
     ]
    }
   ],
   "source": [
    "# call function on neutral review\n",
    "test_review_neu = \"Tahun Pelajaran 2020/2021, Madrasah Gunakan Kurikulum PAI Baru\"\n",
    "predict(net, test_review_neu, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
